\documentclass{homework}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\author{Johann Marques Viana Freitas}
\class{Econometria I: Prof. Nathalie Gimenes}
\date{\today}
\title{Lista Prática}
\address{}

\usepackage[portuguese]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{caption}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document} \maketitle

\question

\section{Funções de Verossimilhança e Estimadores}

\begin{enumerate}
  \item A mistura de normais é caracterizada da seguinte forma:
  \begin{align}
    e_i \sim \begin{cases}
                \mathrm{N}(\mu_1, \sigma_1^2), \text{com probabilidade } 1/2,\\
                \mathrm{N}(\mu_2, \sigma_2^2), \text{com probabilidade } 1/2.
              \end{cases}
  \end{align}
  
  Para computar a densidade, podemos modelar $f(e_i) \sim \mathrm{Bern}(1/2)$ e calcular $\mathbb{E}_{\mathrm{Bern(1/2)}}\left[f(e_i)\right]$:
  
  \begin{align}
    \mathbb{E}_{\mathrm{Bern(1/2)}}\left[f(e_i)\right] &=
    \dfrac{1}{2}\times\left[\dfrac{1}{\sqrt{2\pi\sigma_1^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_1^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_1 \right)^2\right) +
    \dfrac{1}{\sqrt{2\pi\sigma_2^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_2^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_2 \right)^2\right)\right].
  \end{align}
  
  Podemos escrever a função de verossimilhança amostral como
  
  \begin{align}
    L(\bm{\beta}|\mathbf{y}, \mathbf{x}; \mu_1, \mu_2, \sigma_1^2, \sigma_2^2) &= \prod_{i=1}^n\dfrac{1}{2}\times\left[\dfrac{1}{\sqrt{2\pi\sigma_1^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_1^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_1 \right)^2\right) +\right\\
    &\left\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_2^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_2 \right)^2\right)\right] \nonumber\\
    &= \dfrac{1}{2^n}\prod_{i=1}^n\times\left[\dfrac{1}{\sqrt{2\pi\sigma_1^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_1^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_1 \right)^2\right) +\right\nonumber\\
    &\left\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_2^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_2 \right)^2\right)\right]. \nonumber
  \end{align}
  
  Obtendo a log-verossimilhança:
  
  \begin{align}
    l(\bm{\beta}|\mathbf{y}, \mathbf{x}; \mu_1, \mu_2, \sigma_1^2, \sigma_2^2) &= -n\mathrm{log}(2)+\sum^n_{i=1}\mathrm{log}\left(\dfrac{1}{\sqrt{2\pi\sigma_1^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_1^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_1 \right)^2\right) +\right\\
    &\left\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_2^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_2 \right)^2\right)\right)\nonumber.
  \end{align}
  
  O Estimador de Máxima Verossimilhança é dado por
  
  \begin{align}
    \hat{\bm{\beta}} &= \underset{\beta_0, \beta_1}{\mathrm{argmax}}\ l(\bm{\beta}|\mathbf{y}, \mathbf{x}; \mu_1, \mu_2, \sigma_1^2, \sigma_2^2).
  \end{align}
  
  \item Função de verossimilhança amostral para $u$, com $u_i \sim \Gamma(a,b)$, definida para $u_i > 0$:
  \begin{align}
  L(\bm{\beta}|\mathbf{y}, \mathbf{x}; a, b) &= \prod_{i=1}^n{\dfrac{b^a}{\Gamma(a)}\left(y_i - \beta_1 - \beta_2 x_i\right)^{a-1}\mathrm{exp}\left(-b\left(y_i - \beta_1 - \beta_2 x_i\right)\right)}\\
  &= \left(\dfrac{b^a}{\Gamma(a)}\right)^n\prod_{i=1}^n{\left(y_i - \beta_1 - \beta_2 x_i\right)^{a-1}}\mathrm{exp}\left(-b\sum_{i=1}^n\left(y_i - \beta_1 - \beta_2 x_i\right)\right).\nonumber
  \end{align}
  
  Aplicando tranformação logarítmica
  
  \begin{align}
  l(\bm{\beta}|\mathbf{y}, \mathbf{x}; a, b) &= n\mathrm{log}\left(\dfrac{b^a}{\Gamma(a)}\right) + (a-1)\sum_{i=1}^n{\mathrm{log}\left(y_i - \beta_1 - \beta_2 x_i\right)} -b\sum_{i=1}^n\left(y_i - \beta_1 - \beta_2 x_i\right).
  \end{align}
  
  O Estimador de Máxima Verossimilhança é dado por
  
  \begin{align}
    \hat{\bm{\beta}} &= \underset{\beta_0, \beta_1}{\mathrm{argmax}}\ l(\bm{\beta}|\mathbf{y}, \mathbf{x}; a, b).
  \end{align}
  
  \item Sabemos que $v_i \sim \mathrm{Cauchy}(0,1) \sim t(\nu = 1)$, em que $\nu$ representa os graus de liberdade. Portanto, a função de verossimilhança amostral para $\mathbf{v}$, com $v_i \sim \mathrm{Cauchy}(0,1)$ pode ser escrita como:
  
  \begin{align}
    L(\bm{\beta}|\mathbf{y}, \mathbf{x}; \nu) &= \prod_{i=1}^n\dfrac{\Gamma\left(\dfrac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\dfrac{\nu}{2}\right)}\left(1 + \dfrac{\left(y_i - \beta_1 - \beta_2 x_i\right)^2}{\nu}\right)\nonumber\\
    &= \left[\dfrac{\Gamma\left(\dfrac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\dfrac{\nu}{2}\right)}\right]^n\prod_{i=1}^n\left(1 + \dfrac{\left(y_i - \beta_1 - \beta_2 x_i\right)^2}{\nu}\right).
  \end{align}
  
  A log-verossimilhança, por sua vez,
  
  \begin{align}
    l(\bm{\beta}|\mathbf{y}, \mathbf{x}; \nu) &= n\left[\mathrm{log}\left(\Gamma\left(\dfrac{\nu+1}{2}\right)\right)-
    \mathrm{log}\left(\sqrt{\nu\pi}\right)-
    \mathrm{log}\left(\Gamma\left(\dfrac{\nu}{2}\right)\right)\right] + \sum_{i=1}^n\mathrm{log}\left(1 + \dfrac{\left(y_i - \beta_1 - \beta_2 x_i\right)^2}{\nu}\right).
  \end{align}
  
  O Estimador de Máxima Verossimilhança é dado por
  
  \begin{align}
    \hat{\bm{\beta}} &= \underset{\beta_0, \beta_1}{\mathrm{argmax}}\ l(\bm{\beta}|\mathbf{y}, \mathbf{x}; \nu).
  \end{align}

\end{enumerate}

\section{Distribuições Assintóticas}

Vide \cite{greene2011econometric}, Teorema 14.1.M2, \textit{se são satisfeitas condições de regularidade},

\begin{align}
  \hat{\bm{\theta}} \xrightarrow{d} \mathrm{N}\left(\bm{\theta}_0, \{\mathbf{I(\bm{\theta}_0)}\}^{-1}),
\end{align}

em que $\hat{\bm{\theta}}$ é o estimador de Máxima Verossimilhança de $\bm{\theta}_0$, e $\mathbf{I(\bm{\theta}_0)} = -\mathbb{E}\left[\dfrac{\partial^2\mathrm{log}(L)}{\partial\bm{\theta}_0\partial\bm{\theta}_0^'}\right].

\renewcommand{\labelenumi}{R.\arabic{enumi}.}

\begin{tcolorbox}
{\hfill\textbf{Condições de regularidade}\hfill}

\begin{enumerate}

\item As primeiras três derivadas de $\mathrm{log}(f(y_i|\bm{\theta}))$ em relação a $\bm{\theta}$ são contínuas e finitas para quase todo $y_i$, e para todo $\bm{\theta}$.

\item As condições necessárias para obter as esperanças da primeira e da segunda derivada de $\mathrm{log}(f(y_i|\bm{\theta}))$ são satisfeitas.

\item Para todos os valores de $\bm{\theta}$, $\left|\partial^3\mathrm{log}(f(y_i|\bm{\theta}))/\partial\theta_j\partial\theta_k\partial_l\right|$ é menor que uma função com esperança finita.

\end{enumerate}



\end{tcolorbox}

\renewcommand{\labelenumi}{\arabic{enumi})}

\question

\section{Simulações}

O experimento de simulação é conduzido em \verb|R| \cite{rcore}\footnote{Soluções providas por \cite{knitr} e \cite{tidyverse} foram empregadas na análise e reporte deste exercício.}. Primeiramente, definimos parâmetros gerais para a simulação, como o tamanho da amostra e os valores verdadeiros para $\bm{\beta}$.

%\definecolor{shadecolor}{RGB}{255,255,255}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Simulating data generating processes}

\hlstd{n} \hlkwb{=} \hlnum{2000} \hlcom{# Sample size}

\hlcom{# Setting random seed}
\hlkwd{set.seed}\hlstd{(}\hlnum{1981}\hlstd{)}

\hlcom{# Exercise 2}
\hlcom{# Generating x}
\hlstd{x} \hlkwb{=} \hlkwd{rnorm}\hlstd{(}\hlkwc{n} \hlstd{= n,} \hlkwc{mean} \hlstd{=} \hlnum{5}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlkwd{sqrt}\hlstd{(}\hlnum{1}\hlstd{))}

\hlcom{# Defining betas}

\hlstd{beta} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{1.5}\hlstd{,} \hlnum{3}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

Em seguida, simulamos os três processos de geração de dados. Para obter a mistura de normais, é utilizada uma variável auxiliar $P \sim \mathrm{Bernoulli}(1/2)$ e definimos $e_1 \sim \mathrm{N}(\mu_1, \sigma_1^2)$, $e_2 \sim \mathrm{N}(\mu_2, \sigma_2^2)$ e $e = Pe_1 + (1-P)e_2$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Simulating model 1}
\hlcom{# Since \textbackslash{}mu_1, \textbackslash{}mu_2, \textbackslash{}sigma_1 and \textbackslash{}sigma_2 were not specified,}
\hlcom{# I'm considering the following}
\hlstd{mu_1} \hlkwb{=} \hlopt{-}\hlnum{2}
\hlstd{mu_2} \hlkwb{=} \hlnum{2}
\hlstd{sigma_1} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlnum{.5}\hlstd{)}
\hlstd{sigma_2} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlnum{25}\hlstd{)}

\hlstd{e1} \hlkwb{=} \hlkwd{rnorm}\hlstd{(}\hlkwc{n} \hlstd{= n,} \hlkwc{mean} \hlstd{= mu_1,} \hlkwc{sd} \hlstd{= sigma_1)}
\hlstd{e2} \hlkwb{=} \hlkwd{rnorm}\hlstd{(}\hlkwc{n} \hlstd{= n,} \hlkwc{mean} \hlstd{= mu_2,} \hlkwc{sd} \hlstd{= sigma_2)}

\hlcom{# Defining an auxiliary random variable P ~ Bernoulli(1/2)}

\hlstd{P} \hlkwb{=} \hlkwd{rbinom}\hlstd{(}\hlkwc{n} \hlstd{= n,} \hlkwc{size} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{prob} \hlstd{=} \hlnum{1}\hlopt{/}\hlnum{2}\hlstd{)}

\hlstd{e} \hlkwb{=} \hlstd{P} \hlopt{*} \hlstd{e1} \hlopt{+} \hlstd{(}\hlnum{1}\hlopt{-}\hlstd{P)} \hlopt{*} \hlstd{e2}

\hlstd{y_1} \hlkwb{=} \hlstd{beta[}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{beta[}\hlnum{2}\hlstd{]} \hlopt{*} \hlstd{x} \hlopt{+} \hlstd{e}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{tibble}\hlstd{(e1, e2)} \hlopt{%>%}
  \hlkwd{pivot_longer}\hlstd{(}\hlkwc{cols} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{'e1'}\hlstd{,} \hlstr{'e2'}\hlstd{))} \hlopt{%>%}
  \hlkwd{ggplot}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{x} \hlstd{= value,} \hlkwc{group} \hlstd{= name))} \hlopt{+}
    \hlkwd{geom_histogram}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{y} \hlstd{=} \hlkwd{after_stat}\hlstd{(density),} \hlkwc{fill} \hlstd{= name),}
                   \hlkwc{alpha} \hlstd{=} \hlnum{.8}\hlstd{,} \hlkwc{position} \hlstd{=} \hlstr{'identity'}\hlstd{,} \hlkwc{binwidth} \hlstd{=} \hlnum{0.5}\hlstd{,} \hlkwc{color} \hlstd{=} \hlstr{'black'}\hlstd{)} \hlopt{+}
  \hlkwd{labs}\hlstd{(}\hlkwc{fill} \hlstd{=} \hlstr{''}\hlstd{,} \hlkwc{x} \hlstd{=} \hlstr{''}\hlstd{,} \hlkwc{y} \hlstd{=} \hlstr{''}\hlstd{)} \hlopt{+} \hlkwd{scale_fill_manual}\hlstd{(}\hlkwc{values} \hlstd{= purple_rain_colors[}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{3}\hlstd{)])}

\hlkwd{ggplot}\hlstd{()} \hlopt{+}
  \hlkwd{geom_histogram}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{y} \hlstd{=} \hlkwd{after_stat}\hlstd{(density),} \hlkwc{x} \hlstd{= e),}
                 \hlkwc{alpha} \hlstd{=} \hlnum{.8}\hlstd{,} \hlkwc{position} \hlstd{=} \hlstr{'identity'}\hlstd{,} \hlkwc{binwidth} \hlstd{=} \hlnum{0.5}\hlstd{,} \hlkwc{color} \hlstd{=} \hlstr{'black'}\hlstd{,}
                 \hlkwc{fill} \hlstd{= purple_rain_colors[}\hlnum{3}\hlstd{])} \hlopt{+}
  \hlkwd{labs}\hlstd{(}\hlkwc{fill} \hlstd{=} \hlstr{''}\hlstd{,} \hlkwc{x} \hlstd{=} \hlstr{''}\hlstd{,} \hlkwc{y} \hlstd{=} \hlstr{''}\hlstd{)}
\end{alltt}
\end{kframe}\begin{figure}
\subfloat[$e_1$ e $e_2$\label{fig:Simul1-1}]{\includegraphics[width=.49\linewidth]{figure/Simul1-1} }
\subfloat[Mistura\label{fig:Simul1-2}]{\includegraphics[width=.49\linewidth]{figure/Simul1-2} }\caption[Distribuições de $e_1$, $e_2$, $e$]{Distribuições de $e_1$, $e_2$, $e$}\label{fig:Simul1}
\end{figure}

\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Simulating model 2}
\hlcom{# Since a and b were not specified, I'm considering the following}

\hlstd{a} \hlkwb{=} \hlnum{3}
\hlstd{b} \hlkwb{=} \hlnum{6}

\hlstd{u} \hlkwb{=} \hlkwd{rgamma}\hlstd{(}\hlkwc{n} \hlstd{= n,} \hlkwc{shape} \hlstd{= a,} \hlkwc{rate} \hlstd{= b)}

\hlstd{y_2} \hlkwb{=} \hlstd{beta[}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{beta[}\hlnum{2}\hlstd{]} \hlopt{*} \hlstd{x} \hlopt{+} \hlstd{u}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Simulating model 3}

\hlstd{v} \hlkwb{=} \hlkwd{rt}\hlstd{(}\hlkwc{n} \hlstd{= n,} \hlkwc{df} \hlstd{=} \hlnum{1}\hlstd{)}

\hlstd{y_3} \hlkwb{=} \hlstd{beta[}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{beta[}\hlnum{2}\hlstd{]} \hlopt{*} \hlstd{x} \hlopt{+} \hlstd{v}
\end{alltt}
\end{kframe}
\end{knitrout}

\section{Estatísticas descritivas}

\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(}\hlkwd{xtable}\hlstd{(}\hlkwd{map_df}\hlstd{(}\hlkwd{list}\hlstd{(x, e1, e2, e, y_1, u, y_2, v, y_3),} \hlopt{~}\hlkwd{c}\hlstd{(}\hlkwd{summary}\hlstd{(.x),} \hlkwc{Var.} \hlstd{=} \hlkwd{var}\hlstd{(.x)))} \hlopt{%>%}
               \hlstd{\{}\hlkwd{cbind}\hlstd{(}\hlkwd{data.frame}\hlstd{(}\hlstr{'Variable'} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"$\textbackslash{}\textbackslash{}bar\{x\}$"}\hlstd{,} \hlstr{"$\textbackslash{}\textbackslash{}bar\{e\}_1$"}\hlstd{,}
                                                \hlstr{"$\textbackslash{}\textbackslash{}bar\{e\}_2$"}\hlstd{,} \hlstr{"$\textbackslash{}\textbackslash{}bar\{e\}$"}\hlstd{,}
                                                \hlstr{"$\textbackslash{}\textbackslash{}bar\{y\}_1$"}\hlstd{,} \hlstr{"$\textbackslash{}\textbackslash{}bar\{u\}$"}\hlstd{,}
                                                \hlstr{"$\textbackslash{}\textbackslash{}bar\{y\}_2$"}\hlstd{,} \hlstr{"$\textbackslash{}\textbackslash{}bar\{v\}$"}\hlstd{,}
                                                \hlstr{"$\textbackslash{}\textbackslash{}bar\{y\}_3$"}\hlstd{)),.)\},}
      \hlkwc{caption} \hlstd{=} \hlstr{'Estatísticas descritivas'}\hlstd{),}
      \hlkwc{sanitize.text.function}\hlstd{=}\hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)\{x\},} \hlkwc{include.rownames} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\end{kframe}% latex table generated in R 4.1.2 by xtable 1.8-4 package
% Mon May 15 23:40:30 2023
\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrrr}
  \hline
Variable & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max. & Var. \\ 
  \hline
$\bar{x}$ & 1.81 & 4.27 & 4.97 & 4.98 & 5.69 & 8.46 & 1.00 \\ 
  $\bar{e}_1$ & -4.28 & -2.47 & -2.00 & -2.00 & -1.50 & 0.24 & 0.51 \\ 
  $\bar{e}_2$ & -17.73 & -1.44 & 2.20 & 1.97 & 5.41 & 18.62 & 25.27 \\ 
  $\bar{e}$ & -17.73 & -2.36 & -1.48 & -0.02 & 2.29 & 16.15 & 16.76 \\ 
  $\bar{y}_1$ & 0.96 & 12.91 & 15.77 & 16.42 & 19.38 & 37.87 & 25.83 \\ 
  $\bar{u}$ & 0.02 & 0.29 & 0.44 & 0.50 & 0.65 & 1.96 & 0.08 \\ 
  $\bar{y}_2$ & 7.28 & 14.86 & 16.91 & 16.94 & 19.08 & 27.26 & 9.00 \\ 
  $\bar{v}$ & -2163.94 & -1.01 & -0.00 & 1.46 & 0.96 & 5489.95 & 17973.98 \\ 
  $\bar{y}_3$ & -2146.88 & 13.68 & 16.38 & 17.91 & 19.14 & 5501.13 & 17953.97 \\ 
   \hline
\end{tabular}
\caption{Estatísticas descritivas} 
\end{table}


Os momentos teóricos, por sua vez, são os seguintes:

\begin{align}
  \mathbb{E}\left[e_i\right] &= p\mathbb{E}\left[e_{1i}\right] + (1-p)\mathbb{E}\left[e_{2i}\right] = 0\\
  \mathbb{V}\left[e_i\right] &= p(\sigma^2_1 + \mu_1^2) + (1-p)(\sigma^2_2 + \mu_2^2) - [p\mu_1 + (1-p)\mu_2]^2 = 16.75\\
  \mathbb{E}\left[y_{1i}\right] &= \beta_0 + \beta_1 \mathbb{E}\left[x_i\right] + \mathbb{E}\left[e_i\right] = 16.5\\
  \mathbb{V}\left[y_{1i}\right] &= \mathbb{V}\left[\beta_1x_i + e_i\right] = \beta_1^2\mathbb{V}\left[x_{i}\right] + \mathbb{V}\left[e_{i}\right] = 25.75
\end{align}

As estimativas observadas para média e variância amostral de $y_{1}$ são coerentes com a derivação teórica.

\begin{align}
  \mathbb{E}\left[u_i\right] &= \dfrac{a}{b} = 0.5\\
  \mathbb{V}\left[u_i\right] &= \dfrac{a}{b^2} = 0.083333\\
  \mathbb{E}\left[y_{2i}\right] &= \beta_0 + \beta_1 \mathbb{E}\left[x_i\right] + \mathbb{E}\left[u_i\right] = 17\\
  \mathbb{V}\left[y_{2i}\right] &= \mathbb{V}\left[\beta_1x_i + u_i\right] = \beta_1^2\mathbb{V}\left[x_{i}\right] + \mathbb{V}\left[u_{i}\right] = 9.083333
\end{align}

As estimativas observadas para média e variância amostral de $y_{2}$ não diferem substancialmente dos momentos teóricos.

A esperança e a variância para $v_i$ não estão definidas, portanto, também não estão para $y_{3i}$. Como sabemos que a distribuição de $v_i$ é simétrica, bem como a de $x_i$, sugere-se intuitivamente utilizar a mediana como uma medida de centralidade.

\question

\begin{enumerate}

\item Na ausência de conhecimento sobre o Processo Gerador de Dados, pode-se especular a distribuição dos dados a partir da literatura ou argumentações teóricas sobre a variável em questão, bem como o uso de Análise Exploratória de Dados e comparação de medidas de aderência, sem prejuízo da atenção ao sobreajuste, para diferentes especificações. Alternativamente, pode-se optar por um método que não necessite a especificação de uma distribuição de probabilidade. Note que para a especificação 1 são satisfeitas as hipóteses básicas para o Modelo de Regressão Linear e para a consistência do estimador de Mínimos Quadrados Ordinários.

No entanto, para a segunda especificação, como o suporte da distribuição gama é $\left(0,\infty\right)$ e $\mathbb{E}[x_i]$, então seguramente $\mathbb{E}[u_ix_i] \neq 0$. Ou seja, $\hat{\bm{\beta}}_{\mathrm{OLS}} - \bm{\beta} \not\xrightarrow{\mathbb{P}} 0$, embora seja convergente. E o modelo 3, por sua vez, quebra as hipóteses do Modelo Regressão Linear e para a consistência do estimador de Mínimos Quadrados Ordinários pois a esperança e a variância de $v_i$ não são definidas.

\item Definindo a log-verossimilhança amostral, que será a função objetivo do problema de otimização, como função de $\bm{\beta}$ e $\sigma^2$:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Exercise 3.2}
\hlcom{# I'm assuming y - Xb ~ N(0, sigma^2)}
\hlcom{# For the remaining exercises, I'll be using OLS estimates as initial conditions}

\hlcom{# Objective funciton}
\hlstd{l_norm0} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{x}\hlstd{)\{}
  \hlopt{-}\hlkwd{length}\hlstd{(y)}\hlopt{/}\hlnum{2}\hlopt{*}\hlkwd{log}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{pi}\hlopt{*}\hlstd{theta[}\hlnum{3}\hlstd{])} \hlopt{+} \hlkwd{sum}\hlstd{(}\hlopt{-}\hlnum{1}\hlopt{/}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{theta[}\hlnum{3}\hlstd{])}\hlopt{*}\hlstd{(y} \hlopt{-} \hlstd{theta[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{theta[}\hlnum{2}\hlstd{]}\hlopt{*}\hlstd{x)}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Iremos utilizar o algoritmo de Nelder-Mead \cite{nelder1965simplex} para obter uma solução. Para os três modelos será utilizada como condição inicial a estimativa por Mínimos Quadrados Ordinários.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Model 1}

\hlcom{# Initial condition}
\hlstd{init_norm} \hlkwb{=} \hlkwd{lm}\hlstd{(y_1} \hlopt{~} \hlstd{x)} \hlopt{%>%}
  \hlstd{\{}\hlkwd{c}\hlstd{(}\hlkwd{coef}\hlstd{(.),} \hlkwc{sigma2} \hlstd{=} \hlkwd{summary}\hlstd{(.)}\hlopt{$}\hlstd{sigma}\hlopt{^}\hlnum{2}\hlstd{)\}} \hlcom{# Adding OLS estimate for \textbackslash{}sigma^2}

\hlstd{mod1_norm} \hlkwb{<-} \hlkwd{optim}\hlstd{(}\hlkwc{par} \hlstd{= init_norm,}
      \hlkwc{fn} \hlstd{=} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{)\{}\hlopt{-}\hlkwd{l_norm0}\hlstd{(}\hlkwc{theta} \hlstd{= theta,} \hlkwc{y} \hlstd{= y_1,} \hlkwc{x}\hlstd{=x)\})}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(mod1_norm}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## (Intercept)           x      sigma2 
##    1.398715    3.015580   16.756951
\end{verbatim}
\end{kframe}
\end{knitrout}

As estimativas para o modelo 1 aparantam ser, ao menos em termos numéricos, coerentes com os valores populacionais dos parâmetros. Inclusive, $\hat{\sigma^2}$ se aproxima bem das variâncias teórica e amostral de $e_i$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Model 2}

\hlcom{# Initial condition}

\hlstd{init_gamma} \hlkwb{=} \hlkwd{lm}\hlstd{(y_2} \hlopt{~} \hlstd{x)} \hlopt{%>%}
  \hlstd{\{}\hlkwd{c}\hlstd{(}\hlkwd{coef}\hlstd{(.),} \hlkwc{sigma2} \hlstd{=} \hlkwd{summary}\hlstd{(.)}\hlopt{$}\hlstd{sigma}\hlopt{^}\hlnum{2}\hlstd{)\}}

\hlstd{mod2_norm} \hlkwb{<-} \hlkwd{optim}\hlstd{(}\hlkwc{par} \hlstd{= init_gamma,}
      \hlkwc{fn} \hlstd{=} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{)\{}\hlopt{-}\hlkwd{l_norm0}\hlstd{(}\hlkwc{theta} \hlstd{= theta,} \hlkwc{y} \hlstd{= y_2,} \hlkwc{x}\hlstd{=x)\})}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in log(2 * pi * theta[3]): NaNs produzidos}}\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(mod2_norm}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## (Intercept)           x      sigma2 
##  2.04442687  2.99021545  0.08333457
\end{verbatim}
\end{kframe}
\end{knitrout}

A estimativa para $\beta_1$ se aproxima razoavelmente do parâmetro populacional. A estimativa para $\sigma^2$ também é próxima da variância de $u_i \sim \Gamma(a,b)$. No entanto, a estimativa para o intercepto diverge de $\beta_0$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Model 3}

\hlcom{# Initial condition}

\hlstd{init_t} \hlkwb{=} \hlkwd{lm}\hlstd{(y_3} \hlopt{~} \hlstd{x)} \hlopt{%>%}
  \hlstd{\{}\hlkwd{c}\hlstd{(}\hlkwd{coef}\hlstd{(.),} \hlkwc{sigma2} \hlstd{=} \hlkwd{summary}\hlstd{(.)}\hlopt{$}\hlstd{sigma}\hlopt{^}\hlnum{2}\hlstd{)\}}

\hlstd{mod3_norm} \hlkwb{<-} \hlkwd{optim}\hlstd{(}\hlkwc{par} \hlstd{= init_t,}
      \hlkwc{fn} \hlstd{=} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{)\{}\hlopt{-}\hlkwd{l_norm0}\hlstd{(}\hlkwc{theta} \hlstd{= theta,} \hlkwc{y} \hlstd{= y_3,} \hlkwc{x}\hlstd{=x)\})}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(mod3_norm}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
##  (Intercept)            x       sigma2 
##    27.139428    -1.853879 17936.131962
\end{verbatim}
\end{kframe}
\end{knitrout}

A estimativa para $\bm{\beta}$ difere substancialmente do parâmetro populacional. Não é possível comparar $\hat{\sigma^2}$ com $\mathbb{V}[u_i]$ pois esta não é definida.

\item Considerando estimação por Máxima Verossimilhança, pode-se considerar conduzir os testes baseados em Máxima Verossimilhança LR, LM e Wald, assumindo que os modelos satisfaçam as Condições de Regularidade.

\end{enumerate}

\question

\begin{enumerate}

Especificando a função objetivo e estimando os parâmetros para o modelo 1:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Exercise 4}

\hlcom{# Model 1}

\hlcom{# Objective funciton}
\hlstd{l_normmix} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{,} \hlkwc{mu1}\hlstd{,} \hlkwc{mu2}\hlstd{,} \hlkwc{sigma2_1}\hlstd{,} \hlkwc{sigma2_2}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{x}\hlstd{)\{}
  \hlopt{-}\hlkwd{length}\hlstd{(y)}\hlopt{*}\hlkwd{log}\hlstd{(}\hlnum{2}\hlstd{)}\hlopt{+} \hlkwd{sum}\hlstd{(}\hlkwd{log}\hlstd{(}\hlnum{1}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{pi}\hlopt{*}\hlstd{sigma2_1)}\hlopt{*}
                               \hlkwd{exp}\hlstd{(}\hlopt{-}\hlnum{1}\hlopt{/}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{sigma2_1)}\hlopt{*}
                                     \hlstd{(y} \hlopt{-} \hlstd{theta[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{theta[}\hlnum{2}\hlstd{]} \hlopt{*} \hlstd{x} \hlopt{-} \hlstd{mu1)}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{+}
                           \hlnum{1}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{pi}\hlopt{*}\hlstd{sigma2_2)}\hlopt{*}
                           \hlkwd{exp}\hlstd{(}\hlopt{-}\hlnum{1}\hlopt{/}\hlstd{(}\hlnum{2}\hlopt{*}\hlstd{sigma2_2)} \hlopt{*}
                                 \hlstd{(y} \hlopt{-} \hlstd{theta[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{theta[}\hlnum{2}\hlstd{]} \hlopt{*} \hlstd{x} \hlopt{-} \hlstd{mu2)}\hlopt{^}\hlnum{2}\hlstd{)))}
\hlstd{\}}

\hlcom{# Initial condition previously defined #}

\hlstd{mod1_true} \hlkwb{<-} \hlkwd{optim}\hlstd{(}\hlkwc{par} \hlstd{= init_norm[}\hlopt{-}\hlnum{3}\hlstd{],}
      \hlkwc{fn} \hlstd{=} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{)\{}\hlopt{-}\hlkwd{l_normmix}\hlstd{(}\hlkwc{theta} \hlstd{= theta,} \hlkwc{mu1} \hlstd{= mu_1,} \hlkwc{mu2} \hlstd{= mu_2,} \hlkwc{sigma2_1} \hlstd{= sigma_1}\hlopt{^}\hlnum{2}\hlstd{,} \hlkwc{sigma2_2} \hlstd{= sigma_2}\hlopt{^}\hlnum{2}\hlstd{,} \hlkwc{y} \hlstd{= y_1,} \hlkwc{x}\hlstd{=x)\})}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(mod1_true}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## (Intercept)           x 
##    1.684523    2.964243
\end{verbatim}
\end{kframe}
\end{knitrout}

Nota-se que as estimativas, em termos numéricos, não diferem substancialmente das estimativas assumindo distribuição gaussiana. Estas, por sua vez, também são coerentes com os valores populacionais de $\bm{\beta}$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Model 2}

\hlcom{# Objective funciton}
\hlstd{l_gamma} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{,} \hlkwc{a}\hlstd{,} \hlkwc{b}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{x}\hlstd{)\{}
  \hlkwa{if}\hlstd{(}\hlkwd{prod}\hlstd{(y} \hlopt{-} \hlstd{theta[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{theta[}\hlnum{2}\hlstd{]}\hlopt{*}\hlstd{x} \hlopt{>} \hlnum{0}\hlstd{))\{} \hlcom{# Checking whether u_i > 0 \textbackslash{}forall i = 1, ..., n}
    \hlkwd{return}\hlstd{(}\hlkwd{length}\hlstd{(y)}\hlopt{*}\hlkwd{log}\hlstd{(b}\hlopt{^}\hlstd{a}\hlopt{/}\hlkwd{gamma}\hlstd{(a))} \hlopt{+} \hlstd{(a}\hlopt{-}\hlnum{1}\hlstd{)}\hlopt{*}\hlkwd{sum}\hlstd{(}\hlkwd{log}\hlstd{(y} \hlopt{-} \hlstd{theta[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{theta[}\hlnum{2}\hlstd{]}\hlopt{*}\hlstd{x))} \hlopt{-} \hlstd{b}\hlopt{*}\hlkwd{sum}\hlstd{(y} \hlopt{-} \hlstd{theta[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{theta[}\hlnum{2}\hlstd{]} \hlopt{*} \hlstd{x))}
  \hlstd{\}}\hlkwa{else}\hlstd{\{}
    \hlkwd{return}\hlstd{(}\hlopt{-}\hlnum{Inf}\hlstd{)}
    \hlstd{\}}
\hlstd{\}}

\hlcom{# Initial condition previously defined #}

\hlcom{# The objective function evaluated at the initial condition returns an error.}
\hlcom{# So I'll be subtracting 1}

\hlstd{mod2_true} \hlkwb{<-} \hlkwd{optim}\hlstd{(}\hlkwc{par} \hlstd{= init_gamma[}\hlopt{-}\hlnum{3}\hlstd{]}\hlopt{-}\hlnum{1}\hlstd{,}
      \hlkwc{fn} \hlstd{=} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{)\{}\hlopt{-}\hlkwd{l_gamma}\hlstd{(}\hlkwc{theta} \hlstd{= theta,} \hlkwc{a} \hlstd{=} \hlnum{1.75}\hlstd{,} \hlkwc{b} \hlstd{=} \hlnum{.8}\hlstd{,} \hlkwc{y} \hlstd{= y_2,} \hlkwc{x} \hlstd{= x)\})}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(mod2_true}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## (Intercept)           x 
##    1.008164    2.995912
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Model 3}

\hlcom{# Objective funciton}
\hlstd{l_t} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{x}\hlstd{)\{}
  \hlkwd{length}\hlstd{(y)}\hlopt{*}\hlstd{(}\hlkwd{log}\hlstd{(}\hlkwd{gamma}\hlstd{((theta[}\hlnum{3}\hlstd{]}\hlopt{+}\hlnum{1}\hlstd{)}\hlopt{/}\hlnum{2}\hlstd{))} \hlopt{-} \hlkwd{log}\hlstd{(}\hlkwd{sqrt}\hlstd{(theta[}\hlnum{3}\hlstd{]}\hlopt{*}\hlstd{pi))} \hlopt{-} \hlkwd{log}\hlstd{(}\hlkwd{gamma}\hlstd{(theta[}\hlnum{3}\hlstd{]}\hlopt{/}\hlnum{2}\hlstd{)))} \hlopt{-}
    \hlstd{(theta[}\hlnum{3}\hlstd{]}\hlopt{+}\hlnum{1}\hlstd{)}\hlopt{/}\hlnum{2}\hlopt{*}\hlkwd{sum}\hlstd{(}\hlkwd{log}\hlstd{(}\hlnum{1} \hlopt{+} \hlstd{((y} \hlopt{-} \hlstd{theta[}\hlnum{1}\hlstd{]} \hlopt{-} \hlstd{theta[}\hlnum{2}\hlstd{]}\hlopt{*}\hlstd{x)}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlstd{theta[}\hlnum{3}\hlstd{]))}
\hlstd{\}}

\hlstd{init_t2} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlkwd{coef}\hlstd{(}
  \hlkwd{lm}\hlstd{(y_3} \hlopt{~} \hlstd{x)}
\hlstd{),} \hlkwc{df} \hlstd{=} \hlnum{0.0001}\hlstd{)} \hlcom{# Setting a pretty small degree of freedom as an initial guess}

\hlstd{mod3_true} \hlkwb{<-} \hlkwd{optim}\hlstd{(}\hlkwc{par} \hlstd{= init_t2,}
      \hlkwc{fn} \hlstd{=} \hlkwa{function}\hlstd{(}\hlkwc{theta}\hlstd{)\{}\hlopt{-}\hlkwd{l_t}\hlstd{(}\hlkwc{theta} \hlstd{= theta,} \hlkwc{y} \hlstd{= y_3,} \hlkwc{x}\hlstd{=x)\})}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{print}\hlstd{(mod3_true}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## (Intercept)           x          df 
##   1.4711328   3.0079292   0.9809329
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.922, 0.922, 0.922}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{plot_curves} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{...}\hlstd{)\{}
  \hlkwd{ggplot}\hlstd{()} \hlopt{+}
    \hlkwd{geom_point}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{y} \hlstd{=}\hlkwd{eval}\hlstd{(}\hlkwd{parse}\hlstd{(}\hlkwc{text} \hlstd{=} \hlkwd{paste0}\hlstd{(}\hlstr{'y_'}\hlstd{, ...))),} \hlkwc{x}\hlstd{=x),} \hlkwc{alpha} \hlstd{=} \hlnum{.5}\hlstd{,}
               \hlkwc{color} \hlstd{= purple_rain_colors[}\hlnum{1}\hlstd{],} \hlkwc{shape} \hlstd{=} \hlnum{21}\hlstd{)} \hlopt{+}
    \hlkwd{geom_abline}\hlstd{(}\hlkwc{slope} \hlstd{= beta[}\hlnum{2}\hlstd{],}
                \hlkwc{intercept} \hlstd{= beta[}\hlnum{1}\hlstd{],}
                \hlkwc{linetype} \hlstd{=} \hlstr{'F1'}\hlstd{,}
                \hlkwc{color} \hlstd{=} \hlstr{'grey'}\hlstd{,}
                \hlkwc{linewidth} \hlstd{=} \hlnum{1}\hlstd{)} \hlopt{+}
    \hlkwd{geom_abline}\hlstd{(}\hlkwc{slope} \hlstd{=} \hlkwd{eval}\hlstd{(}\hlkwd{parse}\hlstd{(}\hlkwc{text} \hlstd{=} \hlkwd{paste0}\hlstd{(}\hlstr{'mod'}\hlstd{, ...,} \hlstr{'_true$par[2]'}\hlstd{))),}
                \hlkwc{intercept} \hlstd{=} \hlkwd{eval}\hlstd{(}\hlkwd{parse}\hlstd{(}\hlkwc{text} \hlstd{=}\hlkwd{paste0}\hlstd{(}\hlstr{'mod'}\hlstd{, ...,} \hlstr{'_true$par[1]'}\hlstd{))),}
                \hlkwc{linetype} \hlstd{=} \hlstr{'dashed'}\hlstd{,}
                \hlkwc{color} \hlstd{= purple_rain_colors[}\hlnum{1}\hlstd{],}
                \hlkwc{linewidth} \hlstd{=} \hlnum{1}\hlstd{,}
                \hlkwc{alpha} \hlstd{=} \hlnum{.75}\hlstd{)} \hlopt{+}
    \hlkwd{geom_abline}\hlstd{(}\hlkwc{slope} \hlstd{=} \hlkwd{eval}\hlstd{(}\hlkwd{parse}\hlstd{(}\hlkwc{text} \hlstd{=} \hlkwd{paste0}\hlstd{(}\hlstr{'mod'}\hlstd{, ...,} \hlstr{'_norm$par[2]'}\hlstd{))),}
                \hlkwc{intercept} \hlstd{=} \hlkwd{eval}\hlstd{(}\hlkwd{parse}\hlstd{(}\hlkwc{text} \hlstd{=} \hlkwd{paste0}\hlstd{(}\hlstr{'mod'}\hlstd{, ...,} \hlstr{'_norm$par[1]'}\hlstd{))),}
                \hlkwc{linetype} \hlstd{=} \hlstr{'solid'}\hlstd{,}
                \hlkwc{color} \hlstd{= purple_rain_colors[}\hlnum{1}\hlstd{],}
                \hlkwc{linewidth} \hlstd{=} \hlnum{1}\hlstd{,}
                \hlkwc{alpha} \hlstd{=} \hlnum{.75}\hlstd{)} \hlopt{+}
    \hlkwd{labs}\hlstd{(}\hlkwc{x} \hlstd{=} \hlstr{"x"}\hlstd{,} \hlkwc{y} \hlstd{=} \hlstr{"y"}\hlstd{)}
\hlstd{\}}

\hlkwd{invoke_map}\hlstd{(}\hlkwc{.f} \hlstd{= plot_curves,} \hlkwc{.x} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{3}\hlstd{)}
\end{alltt}
\end{kframe}\begin{figure}
\subfloat[Modelo 1\label{fig:curves-1}]{\includegraphics[width=.33333333\linewidth]{figure/curves-1} }
\subfloat[Modelo 2\label{fig:curves-2}]{\includegraphics[width=.33333333\linewidth]{figure/curves-2} }
\subfloat[Modelo 3\label{fig:curves-3}]{\includegraphics[width=.33333333\linewidth]{figure/curves-3} }\caption[Comparativo]{Comparativo}\label{fig:curves}
\end{figure}

\end{knitrout}

\end{enumerate}


% citations
\bibliographystyle{plain}
\bibliography{citations}

\end{document}
