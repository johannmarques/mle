\documentclass{homework}
\author{Johann Marques Viana Freitas}
\class{Econometria I: Prof. Nathalie Gimenes}
\date{\today}
\title{Lista Prática}
\address{}

\usepackage[portuguese]{babel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{caption}

\begin{document} \maketitle

\question

\section{Funções de Verossimilhança e Estimadores}

\begin{enumerate}
  \item A mistura de normais é caracterizada da seguinte forma:
  \begin{align}
    e_i \sim \begin{cases}
                \mathrm{N}(\mu_1, \sigma_1^2), \text{com probabilidade } 1/2,\\
                \mathrm{N}(\mu_2, \sigma_2^2), \text{com probabilidade } 1/2.
              \end{cases}
  \end{align}
  
  Para computar a densidade, podemos modelar $f(e_i) \sim \mathrm{Bern}(1/2)$ e calcular $\mathbb{E}_{\mathrm{Bern(1/2)}}\left[f(e_i)\right]$:
  
  \begin{align}
    \mathbb{E}_{\mathrm{Bern(1/2)}}\left[f(e_i)\right] &=
    \dfrac{1}{2}\times\left[\dfrac{1}{\sqrt{2\pi\sigma_1^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_1^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_1 \right)^2\right) +
    \dfrac{1}{\sqrt{2\pi\sigma_2^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_2^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_2 \right)^2\right)\right].
  \end{align}
  
  Podemos escrever a função de verossimilhança amostral como
  
  \begin{align}
    L(\bm{\beta}|\mathbf{y}, \mathbf{x}; \mu_1, \mu_2, \sigma_1^2, \sigma_2^2) &= \prod_{i=1}^n\dfrac{1}{2}\times\left[\dfrac{1}{\sqrt{2\pi\sigma_1^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_1^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_1 \right)^2\right) +\right\\
    &\left\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_2^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_2 \right)^2\right)\right] \nonumber\\
    &= \dfrac{1}{2^n}\prod_{i=1}^n\times\left[\dfrac{1}{\sqrt{2\pi\sigma_1^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_1^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_1 \right)^2\right) +\right\nonumber\\
    &\left\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_2^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_2 \right)^2\right)\right]. \nonumber
  \end{align}
  
  Obtendo a log-verossimilhança:
  
  \begin{align}
    l(\bm{\beta}|\mathbf{y}, \mathbf{x}; \mu_1, \mu_2, \sigma_1^2, \sigma_2^2) &= -n\mathrm{log}(2)+\sum^n_{i=1}\mathrm{log}\left(\dfrac{1}{\sqrt{2\pi\sigma_1^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_1^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_1 \right)^2\right) +\right\\
    &\left\dfrac{1}{\sqrt{2\pi\sigma_2^2}}\mathrm{exp}\left(-\dfrac{1}{2\sigma_2^2}\left(y_i - \beta_0 - \beta_1 x_i - \mu_2 \right)^2\right)\right)\nonumber.
  \end{align}
  
  O Estimador de Máxima Verossimilhança é dado por
  
  \begin{align}
    \hat{\bm{\beta}} &= \underset{\beta_0, \beta_1}{\mathrm{argmax}}\ l(\bm{\beta}|\mathbf{y}, \mathbf{x}; \mu_1, \mu_2, \sigma_1^2, \sigma_2^2).
  \end{align}
  
  \item Função de verossimilhança amostral para $u$, com $u_i \sim \Gamma(a,b)$, definida para $u_i > 0$:
  \begin{align}
  L(\bm{\beta}|\mathbf{y}, \mathbf{x}; a, b) &= \prod_{i=1}^n{\dfrac{b^a}{\Gamma(a)}\left(y_i - \beta_1 - \beta_2 x_i\right)^{a-1}\mathrm{exp}\left(-b\left(y_i - \beta_1 - \beta_2 x_i\right)\right)}\\
  &= \left(\dfrac{b^a}{\Gamma(a)}\right)^n\prod_{i=1}^n{\left(y_i - \beta_1 - \beta_2 x_i\right)^{a-1}}\mathrm{exp}\left(-b\sum_{i=1}^n\left(y_i - \beta_1 - \beta_2 x_i\right)\right).\nonumber
  \end{align}
  
  Aplicando tranformação logarítmica
  
  \begin{align}
  l(\bm{\beta}|\mathbf{y}, \mathbf{x}; a, b) &= n\mathrm{log}\left(\dfrac{b^a}{\Gamma(a)}\right) + (a-1)\sum_{i=1}^n{\mathrm{log}\left(y_i - \beta_1 - \beta_2 x_i\right)} -b\sum_{i=1}^n\left(y_i - \beta_1 - \beta_2 x_i\right).
  \end{align}
  
  O Estimador de Máxima Verossimilhança é dado por
  
  \begin{align}
    \hat{\bm{\beta}} &= \underset{\beta_0, \beta_1}{\mathrm{argmax}}\ l(\bm{\beta}|\mathbf{y}, \mathbf{x}; a, b).
  \end{align}
  
  \item Sabemos que $v_i \sim \mathrm{Cauchy}(0,1) \sim t(\nu = 1)$, em que $\nu$ representa os graus de liberdade. Portanto, a função de verossimilhança amostral para $\mathbf{v}$, com $v_i \sim \mathrm{Cauchy}(0,1)$ pode ser escrita como:
  
  \begin{align}
    L(\bm{\beta}|\mathbf{y}, \mathbf{x}; \nu) &= \prod_{i=1}^n\dfrac{\Gamma\left(\dfrac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\dfrac{\nu}{2}\right)}\left(1 + \dfrac{\left(y_i - \beta_1 - \beta_2 x_i\right)^2}{\nu}\right)\nonumber\\
    &= \left[\dfrac{\Gamma\left(\dfrac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\dfrac{\nu}{2}\right)}\right]^n\prod_{i=1}^n\left(1 + \dfrac{\left(y_i - \beta_1 - \beta_2 x_i\right)^2}{\nu}\right).
  \end{align}
  
  A log-verossimilhança, por sua vez,
  
  \begin{align}
    l(\bm{\beta}|\mathbf{y}, \mathbf{x}; \nu) &= n\left[\mathrm{log}\left(\Gamma\left(\dfrac{\nu+1}{2}\right)\right)-
    \mathrm{log}\left(\sqrt{\nu\pi}\right)-
    \mathrm{log}\left(\Gamma\left(\dfrac{\nu}{2}\right)\right)\right] + \sum_{i=1}^n\mathrm{log}\left(1 + \dfrac{\left(y_i - \beta_1 - \beta_2 x_i\right)^2}{\nu}\right).
  \end{align}
  
  O Estimador de Máxima Verossimilhança é dado por
  
  \begin{align}
    \hat{\bm{\beta}} &= \underset{\beta_0, \beta_1}{\mathrm{argmax}}\ l(\bm{\beta}|\mathbf{y}, \mathbf{x}; \nu).
  \end{align}

\end{enumerate}

\section{Distribuições Assintóticas}

Vide \cite{greene2011econometric}, Teorema 14.1.M2, \textit{se são satisfeitas condições de regularidade},

\begin{align}
  \hat{\bm{\theta}} \xrightarrow{d} \mathrm{N}\left(\bm{\theta}_0, \{\mathbf{I(\bm{\theta}_0)}\}^{-1}),
\end{align}

em que $\hat{\bm{\theta}}$ é o estimador de Máxima Verossimilhança de $\bm{\theta}_0$, e $\mathbf{I(\bm{\theta}_0)} = -\mathbb{E}\left[\dfrac{\partial^2\mathrm{log}(L)}{\partial\bm{\theta}_0\partial\bm{\theta}_0^'}\right].

\renewcommand{\labelenumi}{R.\arabic{enumi}.}

\begin{tcolorbox}
{\hfill\textbf{Condições de regularidade}\hfill}

\begin{enumerate}

\item As primeiras três derivadas de $\mathrm{log}(f(y_i|\bm{\theta}))$ em relação a $\bm{\theta}$ são contínuas e finitas para quase todo $y_i$, e para todo $\bm{\theta}$.

\item As condições necessárias para obter as esperanças da primeira e da segunda derivada de $\mathrm{log}(f(y_i|\bm{\theta}))$ são satisfeitas.

\item Para todos os valores de $\bm{\theta}$, $\left|\partial^3\mathrm{log}(f(y_i|\bm{\theta}))/\partial\theta_j\partial\theta_k\partial_l\right|$ é menor que uma função com esperança finita.

\end{enumerate}

\renewcommand{\labelenumi}{\arabic{enumi}}

\end{tcolorbox}

\question

\section{Simulações}

O experimento de simulação é conduzido em \verb|R| \cite{rcore}\footnote{Soluções providas por \cite{knitr} e \cite{tidyverse} foram empregadas na análise e reporte deste exercício.}. Primeiramente, definimos parâmetros gerais para a simulação, como o tamanho da amostra e os valores verdadeiros para $\bm{\beta}$.

%\definecolor{shadecolor}{RGB}{255,255,255}

<<LoadTidyverse, echo = FALSE, results='hide', warning = FALSE, message = FALSE>>=
library(tidyverse)
library(knitr)
library(xtable)
library(magrittr)

Sweave2knitr('main.Rnw')

knitr::opts_chunk$set(echo = TRUE, results = FALSE, highlight = TRUE, tidy = TRUE, background = '#ebebeb')

theme_set(theme_bw())

purple_rain_colors <- c(
    "#533a70",
    "#a489c2",
    "#cbbcdc",
    "#e5ddee")
@

<<Preliminaries>>=
# Simulating data generating processes

n = 2000 # Sample size

# Setting random seed
set.seed(1981)

# Exercise 2
# Generating x
x = rnorm(n = n, mean = 5, sd = sqrt(1))

# Defining betas

beta = c(2.35, 0.75)
@

Em seguida, simulamos os três processos de geração de dados. Para obter a mistura de normais, é utilizada uma variável auxiliar $P \sim \mathrm{Bernoulli}(1/2)$ e definimos $e_1 \sim \mathrm{N}(\mu_1, \sigma_1^2)$, $e_2 \sim \mathrm{N}(\mu_2, \sigma_2^2)$ e $e = Pe_1 + (1-P)e_2$.

<<>>=
# Simulating model 1
# Since \mu_1, \mu_2, \sigma_1 and \sigma_2 were not specified,
# I'm considering the following
mu_1 = 2
mu_2 = 1
sigma_1 = sqrt(1.5)
sigma_2 = sqrt(3)

e1 = rnorm(n = n, mean = mu_1, sd = sigma_1)
e2 = rnorm(n = n, mean = mu_2, sd = sigma_2)

# Defining an auxiliary random variable P ~ Bernoulli(1/2)

P = rbinom(n = n, size = 1, prob = 1/2)

e = P * e1 + (1-P) * e2

y_1 = beta[1] + beta[2] * x + e
@

<<Simul1, fig.cap='Distribuições de $e_1$, $e_2$, $e$', fig.subcap=c('$e_1$ e $e_2$', 'Mistura'), out.width='.49\\linewidth'>>=
tibble(e1, e2) %>%
  pivot_longer(cols = c('e1', 'e2')) %>%
  ggplot(aes(x = value, group = name)) +
    geom_histogram(aes(y = after_stat(density), fill = name),
                   alpha = .8, position = 'identity', binwidth = 0.25, color = 'black') +
  labs(fill = '', x = '', y = '') + scale_fill_manual(values = purple_rain_colors[c(1,2)])

ggplot() +
  geom_histogram(aes(y = after_stat(density), x = e),
                 alpha = .8, position = 'identity', binwidth = 0.25, color = 'black',
                 fill = purple_rain_colors[3]) +
  labs(fill = '', x = '', y = '')
@

<<>>=
# Simulating model 2
# Since a and b were not specified, I'm considering the following

a = 1.75
b = .8

u = rgamma(n = n, shape = a, rate = b)

y_2 = beta[1] + beta[2] * x + u
@

<<>>=
# Simulating model 3

v = rt(n = n, df = 1)

y_3 = beta[1] + beta[2] * x + v
@

\section{Estatísticas descritivas}

<<results='asis'>>=
print(xtable(map_df(list(x, e1, e2, y_1, u, y_2, v, y_3), ~summary(.x)) %>%
               {cbind(data.frame('Variable' = c("$\\bar{x}$", "$\\bar{e}_1$",
                                                "$\\bar{e}_2$", "$\\bar{y}_1$",
                                                "$\\bar{u}$", "$\\bar{y}_2$",
                                                "$\\bar{v}$", "$\\bar{y}_3$")),
                      .)},
             ,
      caption = 'Estatísticas descritivas'),
      sanitize.text.function=function(x){x}, include.rownames = FALSE)
@

Os momentos teóricos, por sua vez, são


% citations
\bibliographystyle{plain}
\bibliography{citations}

\end{document}